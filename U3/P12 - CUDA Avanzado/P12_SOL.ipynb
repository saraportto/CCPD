{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a81ee8c",
   "metadata": {},
   "source": [
    "# Tema 12: NVidia CUDA avanzado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04607f",
   "metadata": {},
   "source": [
    "__Ejercicio: sumar filas y columnas de matriz__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def row_sums(a, sums, n):\n",
    "    idx = cuda.grid(1)\n",
    "    sum = 0.0\n",
    "    for i in range(n):\n",
    "        sum += a[idx][i]\n",
    "    sums[idx] = sum\n",
    "\n",
    "@cuda.jit\n",
    "def col_sums(a, sums, n):\n",
    "    idx = cuda.grid(1)\n",
    "    sum = 0.0\n",
    "    for i in range(n):\n",
    "        sum += a[i][idx]\n",
    "    sums[idx] = sum\n",
    "\n",
    "n = 32768 # matrix side size\n",
    "threads_per_block = 256\n",
    "blocks = int(n / threads_per_block)\n",
    "\n",
    "# Input Matrix\n",
    "h_a = np.ones(n*n).reshape(n, n).astype(np.float32)\n",
    "\n",
    "# Vectors in GPU\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_sums = cuda.device_array(shape=(n,), dtype=np.float32)\n",
    "\n",
    "# Calculate sum of rows\n",
    "row_sums[blocks, threads_per_block](d_a, d_sums, n)\n",
    "h_sums = d_sums.copy_to_host()\n",
    "# Check sum\n",
    "truth = h_a.sum(axis=1)\n",
    "np.testing.assert_equal(h_sums,truth)\n",
    "\n",
    "# Calculate sum of columns\n",
    "col_sums[blocks, threads_per_block](d_a, d_sums, n)\n",
    "h_sums = d_sums.copy_to_host()\n",
    "# Check sum\n",
    "truth = h_a.sum(axis=0)\n",
    "np.testing.assert_equal(h_sums,truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit row_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()\n",
    "%timeit col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c8633",
   "metadata": {},
   "source": [
    "__Ejercicio: sumar matrices con kernel 2D__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "n = 4096\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_add(a, b, out, coalesced):\n",
    "    x, y = cuda.grid(2)\n",
    "    if coalesced == True:\n",
    "        out[y][x] = a[y][x]+b[y][x]\n",
    "    else:\n",
    "        out[x][y] = a[x][y]+b[x][y]\n",
    "\n",
    "threads_per_block = (32, 32)  # 2D block\n",
    "blocks = (128, 128) # 2D grid\n",
    "\n",
    "h_a = np.arange(n*n).reshape(n,n).astype(np.float32)\n",
    "h_b = h_a.copy().astype(np.float32)\n",
    "\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_b = cuda.to_device(h_b)\n",
    "d_out = cuda.device_array(shape=(n,n), dtype=np.float32)\n",
    "\n",
    "matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True)\n",
    "h_out = d_out.copy_to_host()\n",
    "truth = h_a+h_b\n",
    "np.testing.assert_equal(h_out, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize()\n",
    "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856a562",
   "metadata": {},
   "source": [
    "__Ejercicio: trasponer matriz con coalescencia usando memoria compartida__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94842075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, types as numba_types\n",
    "import numpy as np\n",
    "n = 4096*4096 # 16M\n",
    "\n",
    "@cuda.jit\n",
    "def transpose(a, transposed):\n",
    "    x, y = cuda.grid(2)\n",
    "    transposed[x][y] = a[y][x]\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose(a, transposed):\n",
    "    tile = cuda.shared.array((32, 32), numba_types.float32)\n",
    "    a_row, a_col = cuda.grid(2)\n",
    "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
    "    cuda.syncthreads()\n",
    "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose2(a, transposed):\n",
    "    tile = cuda.shared.array((32, 33), numba_types.float32)\n",
    "    a_row, a_col = cuda.grid(2)\n",
    "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
    "    cuda.syncthreads()\n",
    "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
    "\n",
    "threads_per_block = (32, 32) # 2D blocks\n",
    "blocks = (128, 128) #2D grid\n",
    "\n",
    "# 4096x4096 input and output matrices\n",
    "h_a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_transposed = cuda.device_array(shape=(4096,4096), dtype=np.float32)\n",
    "d_transposed_alt = cuda.device_array(shape=(4096,4096), dtype=np.float32)\n",
    "\n",
    "\n",
    "# Invocación a traspose y comprobación\n",
    "transpose[blocks, threads_per_block](d_a, d_transposed)\n",
    "result = d_transposed.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(result, expected)\n",
    "\n",
    "# Invocación a tile_traspose y comprobación\n",
    "tile_transpose[blocks, threads_per_block](d_a, d_transposed)\n",
    "result = d_transposed.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(result, expected)\n",
    "\n",
    "# Invocación a tile_traspose2 y comprobación\n",
    "tile_transpose2[blocks, threads_per_block](d_a, d_transposed)\n",
    "result = d_transposed.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(result, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()\n",
    "%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()\n",
    "%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()\n",
    "%timeit tile_transpose2[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
