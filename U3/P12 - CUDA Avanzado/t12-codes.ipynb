{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a81ee8c",
   "metadata": {},
   "source": [
    "# Tema 12: NVidia CUDA avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de95a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar en Google Colab\n",
    "!pip install numpy matplotlib scikit-image numba cython setuptools\n",
    "\n",
    "### EVITAR ERRORES\n",
    "\n",
    "!uv pip install -q --system numba-cuda==0.4.0\n",
    "\n",
    "from numba import config\n",
    "config.CUDA_ENABLE_PYNVJITLINK = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a64185",
   "metadata": {},
   "source": [
    "## A - Warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c46d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks = 1024\n",
    "n = blocks*threads_per_block\n",
    "\n",
    "h_a = np.random.rand(n).astype(np.float32) \n",
    "d_a = cuda.to_device(h_a)\n",
    "\n",
    "@cuda.jit\n",
    "def experiment(a, convergence):\n",
    "    i = cuda.grid(1)\n",
    "    block_idx = cuda.blockIdx.x\n",
    "    if convergence == True:\n",
    "        if (block_idx%4==0):\n",
    "            a[i] = a[i]+2\n",
    "        elif (block_idx%4==1):\n",
    "            a[i] = np.sin(a[i])\n",
    "        elif (block_idx%4==2):\n",
    "            a[i] = np.cos(a[i])\n",
    "        elif (block_idx%4==3):\n",
    "            a[i] = a[i]**0.5\n",
    "    else:\n",
    "        if (i%4==0):\n",
    "            a[i] = a[i]+2\n",
    "        elif (i%4==1):\n",
    "            a[i] = np.sin(a[i])\n",
    "        elif (i%4==2):\n",
    "            a[i] = np.cos(a[i])\n",
    "        elif (i%4==3):\n",
    "            a[i] = a[i]**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit experiment[blocks, threads_per_block](d_a, True); cuda.synchronize()\n",
    "%timeit experiment[blocks, threads_per_block](d_a, False); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c095e11-95e6-4452-b3b5-a3eb0aa701ac",
   "metadata": {},
   "source": [
    "## B - Acceso coalescente a memoria global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65e7b6-b5ef-47e3-9b62-edc08f6383b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "n = 1024*1024 # 10M\n",
    "threads_per_block = 1024\n",
    "blocks = 1024\n",
    "\n",
    "# Input Vectors of length 10M\n",
    "h_a = np.ones(n).astype(np.float32) \n",
    "h_b = h_a.copy().astype(np.float32)\n",
    "\n",
    "# Output Vector\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_b = cuda.to_device(h_b)\n",
    "d_out = cuda.device_array_like(d_a)\n",
    "\n",
    "@cuda.jit\n",
    "def add_experiment(a, b, out, coalesced):\n",
    "    if coalesced == True:\n",
    "        i = cuda.grid(1)\n",
    "        out[i] = a[i] + b[i]\n",
    "    else:\n",
    "        thread_idx = cuda.threadIdx.x\n",
    "        block_idx = cuda.blockIdx.x\n",
    "        out[thread_idx*threads_per_block+block_idx] = \\\n",
    "            a[thread_idx*threads_per_block+block_idx] + \\\n",
    "            b[thread_idx*threads_per_block+block_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416d5e1-5199-49c7-888d-2a5455185329",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize()\n",
    "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04607f",
   "metadata": {},
   "source": [
    "__Ejercicio: sumar filas y columnas de matriz__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def row_sums(a, sums, n):\n",
    "    idx = cuda.grid(1)\n",
    "    sum = 0.0\n",
    "    for i in range(n):\n",
    "        sum += a[idx][i]\n",
    "    sums[idx] = sum\n",
    "\n",
    "# TODO: completar\n",
    "@cuda.jit\n",
    "def col_sums(a, sums, n):\n",
    "    None\n",
    "\n",
    "n = 32768 # matrix side size\n",
    "threads_per_block = 256\n",
    "blocks = int(n / threads_per_block)\n",
    "\n",
    "# Input Matrix\n",
    "h_a = np.ones(n*n).reshape(n, n).astype(np.float32) \n",
    "\n",
    "# Vectors in GPU\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_sums = cuda.device_array(shape=(n,), dtype=np.float32)\n",
    "\n",
    "# Calcular suma de filas\n",
    "row_sums[blocks, threads_per_block](d_a, d_sums, n)\n",
    "h_sums = d_sums.copy_to_host()\n",
    "# Comprobar suma\n",
    "truth = h_a.sum(axis=1)\n",
    "np.testing.assert_equal(h_sums,truth)\n",
    "\n",
    "# TODO: calcular y comprobar suma de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a129883",
   "metadata": {},
   "source": [
    "## C - Kernels bidimensionales y tridimensionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def get_2D_indices(A):\n",
    "    x, y = cuda.grid(2) # Obtenemos las dos dimensiones\n",
    "    # Equivalente a:\n",
    "    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    # Escribimos índice x + '.' + índice y\n",
    "    A[x][y] = x + y / 10\n",
    "\n",
    "d_A = cuda.device_array(shape=(4,4), dtype=np.float32)\n",
    "    # Matriz 4x4 en la GPU\n",
    "\n",
    "blocks = (2, 2) # Grid = 2x2 bloques\n",
    "threads_per_block = (2, 2) # Bloque = 2x2 threads\n",
    "\n",
    "np.set_printoptions(precision=1, floatmode=\"fixed\")\n",
    "get_2D_indices[blocks, threads_per_block](d_A)\n",
    "\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6aaa06",
   "metadata": {},
   "source": [
    "__Ejercicio: sumar matrices con kernel 2D__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "n = 4096\n",
    "\n",
    "# TODO: completar\n",
    "@cuda.jit\n",
    "def matrix_add(a, b, out, coalesced):\n",
    "    None\n",
    "\n",
    "threads_per_block = (32, 32)  # 2D block\n",
    "blocks = (128, 128) # 2D grid\n",
    "\n",
    "h_a = np.arange(n*n).reshape(n,n).astype(np.float32)\n",
    "h_b = h_a.copy().astype(np.float32)\n",
    "\n",
    "# TODO: copia a la GPU y reserva para la salida\n",
    "\n",
    "# TODO: invocación kernel y obtención resultados\n",
    "truth = h_a+h_b\n",
    "np.testing.assert_equal(h_out, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d18255",
   "metadata": {},
   "source": [
    "## D - Memoria compartida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import types, cuda\n",
    "\n",
    "n=5\n",
    "\n",
    "@cuda.jit\n",
    "def swap_with_shared(vector, swapped):\n",
    "    temp = cuda.shared.array(n, dtype=types.int32)\n",
    "    idx = cuda.grid(1)\n",
    "    temp[idx] = vector[idx]\n",
    "\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    swapped[idx] = temp[n-1 - cuda.threadIdx.x]\n",
    "\t\t# swap elements\n",
    "\n",
    "h_vector = np.arange(n).astype(np.int32)\n",
    "print(\"Antes:\",h_vector)\n",
    "h_swapped = np.zeros_like(h_vector)\n",
    "\n",
    "d_vector = cuda.to_device(h_vector)\n",
    "d_swapped = cuda.device_array(shape=(n,), dtype=np.int32)\n",
    "\n",
    "swap_with_shared[1, n](d_vector, d_swapped)\n",
    "result = d_swapped.copy_to_host()\n",
    "print(\"Después:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de4db",
   "metadata": {},
   "source": [
    "__Ejercicio: trasponer matriz con coalescencia usando memoria compartida__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, types as numba_types\n",
    "import numpy as np\n",
    "n = 4096*4096 # 16M\n",
    "\n",
    "@cuda.jit\n",
    "def transpose(a, transposed):\n",
    "    x, y = cuda.grid(2)\n",
    "    transposed[x][y] = a[y][x]\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose(a, transposed):\n",
    "    # Suponer bloques 32x32\n",
    "\n",
    "    # TODO 1: crear array 32x32 en memoria compartida\n",
    "\n",
    "    # Calcular índices en array a\n",
    "    a_row, a_col = cuda.grid(2)\n",
    "   \n",
    "    # TODO 2: leer de memoria global y escribir en memoria compartida\n",
    "    #   los índices locales serán para el buffer en memoria compartida\n",
    "    #   y los globales para el array a\n",
    "    \n",
    "    # TODO 3: Esperamos a que todos los hilos del bloque actualicen la escritura\n",
    " \n",
    "    t_row, t_col = cuda.grid(2)\n",
    "    # TODO 4: Escribir de memoria compartida (usando índices de hilo)\n",
    "    #   a memoria global (usando índices de grid) transponiendo cada elemento\n",
    "    \n",
    "threads_per_block = (32, 32) # 2D blocks\n",
    "blocks = (128, 128) #2D grid\n",
    "\n",
    "# 4096x4096 input and output matrices\n",
    "h_a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_transposed = cuda.device_array(shape=(4096,4096), dtype=np.float32)\n",
    "\n",
    "# Invocación a traspose y comprobación\n",
    "transpose[blocks, threads_per_block](d_a, d_transposed)\n",
    "result = d_transposed.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(result, expected)\n",
    "\n",
    "# Invocación a tile_traspose y comprobación\n",
    "tile_transpose[blocks, threads_per_block](d_a, d_transposed)\n",
    "result = d_transposed.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f65207",
   "metadata": {},
   "source": [
    "## F - Comparación accesos a memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cef806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, cuda, types as numba_types\n",
    "import numpy as np\n",
    "\n",
    "n = 1024*1024 # 1M\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "\n",
    "@jit\n",
    "def transpose_CPU(a, transposed):\n",
    "    for i in range(1024):\n",
    "        for j in range(1024):\n",
    "            transposed[i,j] = a[j,i]\n",
    "\n",
    "transpose_CPU(h_a, h_out)\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(h_out, expected)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
    "\n",
    "@cuda.jit\n",
    "def transpose1thread(a, transposed):\n",
    "    for i in range(1024):\n",
    "        for j in range(1024):\n",
    "            transposed[i,j] = a[j,i]\n",
    "\n",
    "transpose1thread[1, 1](d_a, d_out)\n",
    "expected = h_a.T\n",
    "h_out=d_out.copy_to_host()\n",
    "np.testing.assert_equal(h_out, expected)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
    "\n",
    "@cuda.jit\n",
    "def transpose1024thread(a, transposed):\n",
    "    i = cuda.threadIdx.x\n",
    "    for j in range(1024):\n",
    "        transposed[i,j] = a[j,i]\n",
    "\n",
    "transpose1024thread[1, 1024](d_a, d_out)\n",
    "expected = h_a.T\n",
    "h_out=d_out.copy_to_host()\n",
    "np.testing.assert_equal(h_out, expected)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
    "\n",
    "@cuda.jit\n",
    "def transpose1Mthreads(a, transposed):\n",
    "    (i,j) = cuda.grid(2)\n",
    "    transposed[i,j] = a[j,i]\n",
    "\n",
    "blocks=(32,32)\n",
    "threads_per_block=(32,32)\n",
    "transpose1Mthreads[blocks, threads_per_block](d_a, d_out)\n",
    "h_out=d_out.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(h_out, expected)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
    "\n",
    "nthreads=32\n",
    "nblocks=int(1024/nthreads)\n",
    "@cuda.jit\n",
    "def tile_transpose(a, transposed):\n",
    "    tile = cuda.shared.array((nthreads, nthreads), numba_types.float32)\n",
    "    a_row, a_col = cuda.grid(2)\n",
    "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
    "    cuda.syncthreads()\n",
    "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
    "\n",
    "blocks=(nblocks,nblocks)\n",
    "threads_per_block=(nthreads, nthreads)\n",
    "tile_transpose[blocks, threads_per_block](d_a, d_out)\n",
    "h_out=d_out.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(h_out, expected)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
    "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
    "\n",
    "nthreads=32\n",
    "nblocks=int(1024/nthreads)\n",
    "ncols=33\n",
    "@cuda.jit\n",
    "def tile_transpose2(a, transposed):\n",
    "    tile = cuda.shared.array((nthreads, ncols), numba_types.float32)\n",
    "    a_row, a_col = cuda.grid(2)\n",
    "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
    "    cuda.syncthreads()\n",
    "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
    "\n",
    "blocks=(nblocks,nblocks)\n",
    "threads_per_block=(nthreads, nthreads)\n",
    "tile_transpose[blocks, threads_per_block](d_a, d_out)\n",
    "h_out=d_out.copy_to_host()\n",
    "expected = h_a.T\n",
    "np.testing.assert_equal(h_out, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose_CPU(h_a, h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose1thread[1, 1](d_a, d_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose1024thread[1, 1024](d_a, d_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit transpose1Mthreads[(32,32), (32,32)](d_a, d_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tile_transpose[(32,32), (32,32)](d_a, d_out); cuda.synchronize()\n",
    "%timeit tile_transpose[(64,64), (16,16)](d_a, d_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tile_transpose2[(32,32), (32,32)](d_a, d_out); cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
